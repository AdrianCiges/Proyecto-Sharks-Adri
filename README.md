#  SHARKS DATA BASE ğŸ¦ˆ <br />
## CONTENIDO ğŸ“‘
[1 - Objetivo ğŸ¯](#O)<br />
[2 - ExploraciÃ³n ğŸ‘€](#E) <br />
[3 - Correcciones Iniciales ğŸ”§](#CI)<br />
[4 - Limpieza ğŸ§¼](#L)<br />
[5 - AnÃ¡lisis ğŸ“Š](#A)<br />
[6 - Links y Recursos ğŸ’»](#LR)<br />

## OBJETIVO <a name="O"/> 
ğŸ§¹ Limpiar la base de datos lo mÃ¡s minuciosamente posible con un deadline de 3 dÃ­as.<br />

ğŸ“Š Realizar un pequeÃ±o anÃ¡lisis de los datos.<br />

ğŸ”Š Exponer las conclusiones obtenidas.

## EXPLORACIÃ“N ğŸ‘€ <a name="E"/> 
â¤µï¸ Importamos y leemos el archivo.<br />

ğŸ“… Convertimos en dataframe.<br />

ğŸ” Observamos tipos de variables, nulos, cantidad de registros, valores extraÃ±os o inconsistentes, etc.

## CORRECCIONES INICIALES ğŸ”§ <a name="CI"/> 
ğŸª› Corregimos los nombres de las columnas (espacios, carÃ¡cteres especiales...)<br />

â€¼ï¸ Eliminamos duplicados (19441 duplicados).<br />

â›” Observamos columnas con muchos nulos -> No se nos permite eliminar columnas, pero eliminarÃ­amos aquellas con +80% nulos.<br />

ğŸš© Eliminamos filas con muchos nulos (drop 10 filas). <br />

ğŸ”ƒ Corregimos valores nulos (observamos nulos por columna).<br />

ğŸ…°ï¸ Convertimos todos los valores a MAYÃšSCULAS para evitar problemas.<br />

ğŸ”˜ Quitamos espacios iniciales y finales de todos los valores.<br />

ğŸš¨ Seleccionamos columnas (variables) de interÃ©s (col_vip).<br />

ğŸ” Creamos subset con esas columnas (Â¿duplicados? Solo 1, pero eliminamos).

## LIMPIEZA ğŸ§¼ <a name="L"/> 
ğŸ§¹ Limpiamos las columnas importantes (Case_number [Fecha*], Type, Country, Area, Location, Activity, Sex, Age, Fatal(y/n), Species).<br />

ğŸ—“ï¸ Renombramos "Case_Number" por "Fecha".<br />

ğŸŒ Pasamos a "Unknown" los PaÃ­ses y las Ãreas con pocos casos (ruido, aportaciÃ³n residual).<br />

ğŸ”™ Decidimos quitar "Location" de las columnas importantes (aportaciÃ³n residual).<br />

ğŸ” Creamos un nuevo subset con las columnas importantes.<br />

ğŸ—‘ï¸ Eliminamos filas con muchos valores "Unknown" (criterio >3, 1655 filas).<br />

ğŸ—‘ï¸ Eliminamos filas que supongan duplicados en el subset (49 filas).<br />

ğŸ—‘ï¸ Eliminamos datos anteriores al aÃ±o 1900 por inconsistencia del dato (316 filas).<br />

â¤´ï¸ Exportamos a excel y analizamos.<br />

## ANÃLISIS ğŸ“Š <a name="A"/> 
ğŸš» Ataques por Sexo: 88/12 % (H/M).<br />

ğŸ‘§ Ataques por Edad: Top -> JÃ³venes y peso relativo NiÃ±as y Ancianas vs Hombres.<br />

ğŸ¦ˆ Ataques por Especie: Top -> TiburÃ³n Blanco.<br />

ğŸ’€ Letalidad por Especie: Top -> TiburÃ³n Blanco.<br />

ğŸ„ Ataques por Actividad: Top -> Surf.<br />

ğŸŠ Letalidad por Actividad: Top -> Nado.<br />

ğŸ—ºï¸ Ataques por PaÃ­s: Top -> EEUU.<br />

ğŸ“ˆ Letalidad por PaÃ­s: Top -> Australia.

## LINKS Y RECURSOS ğŸ’» <a name="LR"/>
â€¢ https://www.kaggle.com/teajay/global-shark-attacks <br />
â€¢ https://numpy.org/doc/1.18/ <br />
â€¢ https://pandas.pydata.org/ <br />
â€¢ https://docs.python.org/3/library/functions.html <br />
â€¢ https://plotly.com/python/ <br />
â€¢ https://matplotlib.org/ <br />
â€¢ https://seaborn.pydata.org/ <br />
â€¢ https://pandas.pydata.org/docs/ <br />
â€¢ https://towardsdatascience.com/beware-of-storytelling-with-data-1710fea554b0?gi=537e0c10d89e
