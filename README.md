# Proyecto-Sharks-Adri ðŸ¦ˆ <br />
## ðŸ‘€ ExploraciÃ³n
â€¢ Importamos y leemos el archivo.<br />
â€¢ Convertimos en dataframe.<br />
â€¢ Observamos tipos de variables, nulos, cantidad de registros, valores extraÃ±os o inconsistentes, etc.

## ðŸ”§ Correcciones iniciales
â€¢ Corregimos los nombres de las columnas (espacios, carÃ¡cteres especiales...)<br />
â€¢ Eliminamos duplicados (19441 duplicados)<br />
â€¢ Eliminamos filas con muchos nulos (drop 10 filas)<br />
â€¢ Corregimos valores nulos (observamos nulos por columna)<br />
â€¢ Corregimos los valores "object" por "unknown")<br />
â€¢ Convertimos todos los valores a MAYÃšSCULAS para evitar problemas.<br />
â€¢ Quitamos espacios iniciales y finales de todos los valores.<br />
â€¢ Seleccionamos columnas (variables) de interÃ©s (col_vip).<br />
â€¢ Creamos subset con esas columnas (Â¿duplicados? Solo 1, pero eliminamos)

Limpiamos las columnas importantes (Case_number [Fecha*], Type, Country, Area, Location, Activity, Sex, Age, Fatal(y/n), Species).

Renombramos "Case_Number" por "Fecha".

Pasamos a "Unknown" los paises con pocos casos (ruido, aportaciÃ³n residual).

Hacemos lo mismo con el "Area".

Decidimos quitar "Location" de las columnas importantes (aportaciÃ³n residual).

Creamos un nuevo subset con las columnas importantes.

Eliminamos filas con muchos valores "Unknown" (criterio >3, 1655 filas).

Eliminamos filas que supongan duplicados en el subset (49 filas).

Eliminamos datos anteriores al aÃ±o 1900 por inconsistencia del dato (316 filas).

Exportamos a excel y analizamos. 
